{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = 'https://te.wikipedia.org'\n",
    "links = ['https://te.wikipedia.org/w/index.php?title=%E0%B0%AA%E0%B1%8D%E0%B0%B0%E0%B0%A4%E0%B1%8D%E0%B0%AF%E0%B1%87%E0%B0%95:%E0%B0%85%E0%B0%A8%E0%B1%8D%E0%B0%A8%E0%B0%BF%E0%B0%AA%E0%B1%87%E0%B0%9C%E0%B1%80%E0%B0%B2%E0%B1%81&from=2014+%E0%B0%86%E0%B0%82%E0%B0%A7%E0%B1%8D%E0%B0%B0%E0%B0%AA%E0%B1%8D%E0%B0%B0%E0%B0%A6%E0%B1%87%E0%B0%B6%E0%B1%8D+%E0%B0%B8%E0%B0%BE%E0%B0%B0%E0%B1%8D%E0%B0%B5%E0%B0%A4%E0%B1%8D%E0%B0%B0%E0%B0%BF%E0%B0%95+%E0%B0%8E%E0%B0%A8%E0%B1%8D%E0%B0%A8%E0%B0%BF%E0%B0%95%E0%B0%B2%E0%B1%81']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_buffer(response):\n",
    "    response.text = response.read()\n",
    "    return response.text.decode('utf-8')\n",
    "\n",
    "def get_data_from_url(url):\n",
    "    try:\n",
    "        r = urlopen(url)\n",
    "        doc = readout_buffer(r)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        doc = \"\"\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99089, 99052)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links = []\n",
    "# Main code\n",
    "prev_len = 0\n",
    "for link in links:  \n",
    "    while link:\n",
    "        html_doc = ''\n",
    "        # open the main link\n",
    "        doc = get_data_from_url(link)\n",
    "        # parse html\n",
    "        soup = BeautifulSoup(doc, 'html.parser')\n",
    "        # filter body\n",
    "        div = soup.find('div',{'class':'mw-allpages-body'})\n",
    "        # find all ahref tags\n",
    "        if div:\n",
    "            anchors = div.find_all('a');\n",
    "            all_links = all_links + [home_url + anchor['href'] for anchor in anchors]\n",
    "        # if no hrefs found break the loop\n",
    "        if prev_len == len(set(all_links)):\n",
    "            break\n",
    "        # find the navigation div\n",
    "        nav_div = soup.find('div',{'class':'mw-allpages-nav'})\n",
    "        if nav_div and len(nav_div.find_all('a')) == 2:\n",
    "            link = home_url + nav_div.find_all('a')[1]['href']\n",
    "        prev_len = len(set(all_links))\n",
    "        \n",
    "len(all_links), len(set(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B0%BE%E0%B0%AE%E0%B0%95%E0%B1%83%E0%B0%B7%E0%B1%8D%E0%B0%A3_%E0%B0%AE%E0%B0%A0%E0%B0%AE%E0%B1%81,_%E0%B0%B9%E0%B1%88%E0%B0%A6%E0%B0%B0%E0%B0%BE%E0%B0%AC%E0%B0%BE%E0%B0%A6%E0%B1%81'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_links = list(set(all_links));\n",
    "unique_links[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(unique_links, columns = ['link'])\n",
    "df.to_csv('telugu_wiki_links.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Pages from Links & Parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selectolax\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/6d/ad7ae4b4be8d43799019d5d4312b82cddf2540bc4334be6c327d8d7dc6c4/selectolax-0.2.3-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 2.2MB/s \r\n",
      "\u001b[?25hInstalling collected packages: selectolax\r\n",
      "Successfully installed selectolax-0.2.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selectolax\n",
    "from selectolax.parser import HTMLParser\n",
    "def get_details(url):\n",
    "    doc = get_data_from_url(url)\n",
    "    try: \n",
    "        html_doc = HTMLParser(doc)\n",
    "        t = '\\n '.join(n.text() for n in html_doc.css(\"title\"))\n",
    "        a = '\\n '.join(n.text() for n in html_doc.css(\"p\"))\n",
    "    except:\n",
    "        t = \"\"\n",
    "        a = \"\"\n",
    "    return t, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelising the task on 4 cpu cores\n",
      "Done for 10000 rows ---> 0:09:35.897648\n",
      "Done for 20000 rows ---> 0:19:05.904842\n",
      "Done for 30000 rows ---> 0:28:34.470181\n",
      "Done for 40000 rows ---> 0:38:00.445309\n",
      "Done for 50000 rows ---> 0:47:26.622419\n",
      "Done for 60000 rows ---> 0:56:52.028604\n",
      "Done for 70000 rows ---> 1:06:09.112798\n",
      "Done for 80000 rows ---> 1:15:30.016545\n",
      "Done for 90000 rows ---> 1:24:49.169892\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import multiprocessing.dummy as mpd\n",
    "import time\n",
    "\n",
    "start = datetime.now()\n",
    "cpu_cores = mp.cpu_count()\n",
    "print('parallelising the task on {} cpu cores'.format(cpu_cores))\n",
    "\n",
    "rows = []\n",
    "count = 0\n",
    "# divide pool\n",
    "pool = mpd.Pool(processes=cpu_cores)\n",
    "# iter over \n",
    "for row in pool.imap(get_details, unique_links):\n",
    "    rows.append(row)    \n",
    "    count = count + 1\n",
    "    # print/save\n",
    "    if not count%10000:\n",
    "        df = pd.DataFrame(rows, columns = ['title', 'text'])\n",
    "        df.to_parquet('telugu_wikipedia_dataset.parquet', index = None)\n",
    "        print(\"Done for {} rows ---> {}\".format(count, datetime.now() - start))\n",
    "# close the pool\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99052, 2), Index(['title', 'text'], dtype='object'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows, columns = ['title', 'text'])\n",
    "df.to_parquet('telugu_wikipedia_dataset.parquet')\n",
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>వేములకొండ - వికీపీడియా</td>\n",
       "      <td>\\n\\n \\n\\n వేములకొండ, తూర్పు గోదావరి జిల్లా, రం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>గంగవరం (కోవూరు) - వికీపీడియా</td>\n",
       "      <td>గంగవరం ఆంధ్ర ప్రదేశ్ రాష్ట్రం, శ్రీ పొట్టి శ్ర...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>అన్నదమ్ముల శపధం - వికీపీడియా</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>సోమిదేవిపల్లి - వికీపీడియా</td>\n",
       "      <td>సోమిదేవిపల్లి, ప్రకాశం జిల్లా, రాచర్ల మండలానిక...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>పెద్దపాడు (శ్రీకాకుళం మండలం) - వికీపీడియా</td>\n",
       "      <td>పెద్దపాడు శ్రీకాకుళం జిల్లా, శ్రీకాకుళం మండలం ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                     వేములకొండ - వికీపీడియా   \n",
       "1               గంగవరం (కోవూరు) - వికీపీడియా   \n",
       "2               అన్నదమ్ముల శపధం - వికీపీడియా   \n",
       "3                 సోమిదేవిపల్లి - వికీపీడియా   \n",
       "4  పెద్దపాడు (శ్రీకాకుళం మండలం) - వికీపీడియా   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\n \\n\\n వేములకొండ, తూర్పు గోదావరి జిల్లా, రం...  \n",
       "1  గంగవరం ఆంధ్ర ప్రదేశ్ రాష్ట్రం, శ్రీ పొట్టి శ్ర...  \n",
       "2                                                     \n",
       "3  సోమిదేవిపల్లి, ప్రకాశం జిల్లా, రాచర్ల మండలానిక...  \n",
       "4  పెద్దపాడు శ్రీకాకుళం జిల్లా, శ్రీకాకుళం మండలం ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_details(doc):\n",
    "# \"\"\"\n",
    "# super slow parsing\n",
    "# \"\"\"\n",
    "#     try: \n",
    "#         soup = BeautifulSoup(doc, 'html.parser')\n",
    "#         # print(soup.title.string)\n",
    "#         paras = soup.find_all('p')\n",
    "#         a = ' \\n'.join([para.text for para in paras])\n",
    "#         t = soup.title.string\n",
    "#     except:\n",
    "#         t = \"\"\n",
    "#         a = \"\"\n",
    "#     return t, a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
